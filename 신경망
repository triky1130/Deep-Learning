원하는 결과를 출력하도록 가중치 값을 적절히 정하는 작업은 인간의 수동적인 몫이다.
신경망은 가중치 매개변수의 적절한 값을 데이터로부터 자동으로 학습하는 능력을 가졌다.
신경망은 입력층, 은닉층, 출력층으로 나뉜다.

#활성화 함수의 등장
입력 신호의 총합을 출력 신호로 변환하는 함수
ex) 퍼셉트론
y = h(b + w1x1 + w2x2) ->편향을 포함한 입력신호의 값을 h(x), 즉 활성화 함수로 표현한 것
h(x) = 0 (x <=0)
h(x) = 1 (x > 0)
이와 같이 임계값을 경계로 출력이 바뀌는데 이런 함수를 계단 함수(step function)이라 합니다.
여기서 계단함수 이외에 다른 함수를 사용한다면 신경망의 세계로 나아가는 열쇠이다.

#시그모이드 함수
h(x) = 1 / 1 + exp(-x)
exp(-x) = e^-x 의미, 복잡해보이지만 입력을 주면 출력을 돌려주는 변환기이다. 

신경망에서는 활성화 함수로 시그모이드 함수를 이용하여 신호를 변환하고, 그 변환된 신호를 다음 뉴런에 전달합니다.
사실상 퍼셉트론과 신경망의 주된 차이는 활성화 함수 뿐이다.

#계단 함수 구현하기
def step_function(x):
  if x > 0:
    return 1
  else:
    return 0
    
+넘파이 배열을 인수로 넣기 위한 계단 함수 구현
def step_fuction(x):
  y = x > 0
  return y.astype(np.int)
 
import numpy as np
x = np.arrat([-1.0, 1.0, 2.0])
y = x > 0 #부등호 연산 수행 시
y
>>array([False, True, True], dtype = bool) #bool 타입 말고 int 타입으로 출력해야 계단 함수 출력됨
y = y.astype(np.int)
y
>>array([0, 1, 1])

#계단 함수의 그래프
import numpy as np
import matplotlib.pylab as plt

def step_function(x):
  return np.array(x > 0, dtype = np.int)
  
x = np.arange(-5.0, 5.0, 0.1) #-5.0에서 5.0 전까지 0.1간격의 넘파이 배열 생성
y = step_function(x)
plt.plot(x, y)
plt.ylim(-0.1, 1.1) #y축의 범위 지정
plt.show()

결과: 0을 경계로 출력이 0에서 1로 바뀐다.

#시그모이드 함수 구현하기

def sigmoid(x):
  return 1 / (1 + np.exp(-x))
x = np.array([-1.0, 1.0, 2.0])
sigmoid(x)
>>array([ 0.26894142, 0.73105858, 0.88079708])

인수가 넘파이 배열이어도 결과가 나온다. 이는 넘파이의 브로드캐스트 덕분.

x = np.arange(-5.0, 5.0, 0.1) #-5.0에서 5.0 전까지 0.1간격의 넘파이 배열 생성
y = sigmoid(x)
plt.plot(x, y)
plt.ylim(-0.1, 1.1) #y축의 범위 지정
plt.show()

결과: S자 모양의 그래프 생성

#시그모이드 함수와 계단 함수 비교
계단 함수는 0과 1 중 하나의 값만 돌려주는 반면 시그모이드 함수는 실수를 돌려준다.
따라서 매끄러움에 차이가 있다. (=연속적인 실수가 흐른다)
공통점: 입력이 작을 때의 출력은 0에 가깝고 입력이 커지면 1에 가까워진다. 비선형 함수이다.

#비선형 함수
신경망에서는 활성화 함수로 비선형 함수를 이용해야 한다. 선형 함수를 사용하면 신경망의 층을 깊게 하는 이유가 없다.
예를 들어 h(x) = cx를 활성화 함수로 사용한다면 3층 네트워크를 구현할 때
y(x) = h(h(h(x)))이며 결국의 형태는 y = ax이다. 그러면 은닉층이 없는 네트워크가 된다. 

#ReLU 함수
Rectified Linear Unit function
ReLU는 입력이 0을 넘으면 그 입력을 그대로 출력하고, 0 이하이면 0을 출력하는 함수이다.
h(x) = x(x > 0)
h(x) = 0(x <= 0)

def relu(x):
  return np.maximum(0, x) #maximum 함수는 두 입력 중 큰 값을 선택해 반환하는 함수이다.
  
#다차원 배열의 계산
넘파이의 다차원 배열 계산이 숙달될수록 신경망을 효율적으로 구현할 수 있다.

import numpy as np
A = np.array([1, 2, 3, 4])
print(A)
>>[1 2 3 4]
np.ndim(A) #차원을 물어보는 함수 ndim
>>1
A.shape #배열의 형상을 물어보면 튜플로 알려줌
>>(4, ) #원소 4개로 구성된 1차원 배열이라는 의미
A.shape[0]
>>4
B = np.array([[1, 2], [3, 4], [5, 6]])
print(B)
>>[[1, 2]
   [3, 4]
   [5, 6]]
np.ndim(B)
>>2
B.shape
>>(3,2) #배열이 각 원소이고 원소가 3개로 구성된 2차원 배열이라는 의미

#행렬의 곱
A  = np.array([[1, 2], [3, 4]])
B  = np.array([[5, 6], [7, 8]])
np.dot(A, B) #행렬 곱 계산 함수
>>array([[19, 22]
         [43, 50]])

!행렬 A의 1번째 차원의 원소 수(열 수)와 행렬 B의 0번째 차원의 원소 수(행 수)가 같아야한다!
A = np.array([[1, 2, 3], [4, 5, 6]])
B = np.array([[1, 2], [3, 4], [5, 6]])
C = np.arrray([[1, 2], [3, 4]]) 
np.dot(A, B)는 가능
np.dot(A, C)는 불가능 #A의 원소 수는 3, C의 행 수는 2라서

#신경망에서의 행렬 곱(p82)
